---
title: Resources
permalink: /resources/
---

## Platforms
<hr>

### 1. MARLlib
**Multi-agent Reinforcement Learning Library ([MARLlib](https://arxiv.org/abs/2210.13708))** is ***a MARL library*** that utilizes [**Ray**](https://github.com/ray-project/ray) and one of its toolkits [**RLlib**](https://github.com/ray-project/ray/tree/master/rllib). It offers a comprehensive platform for developing, training, and testing MARL algorithms across various tasks and environments. 

<img width="1508" alt="image" src="https://github.com/Replicable-MARL/MARLlib/blob/docs/docs/source/images/reler_marllib_source.png?raw=true">

Here's an example of MARLlib's API usage:

```py
from marllib import marl

# prepare env
env = marl.make_env(environment_name="mpe", map_name="simple_spread", force_coop=True)

# initialize algorithm with appointed hyper-parameters
mappo = marl.algos.mappo(hyperparam_source='mpe')

# build agent model based on env + algorithms + user preference
model = marl.build_model(env, mappo, {"core_arch": "mlp", "encode_layer": "128-256"})

# start training
mappo.fit(env, model, stop={'timesteps_total': 1000000}, share_policy='group')
```


## Datasets
<hr>

### 1. University-1652: A Multi-view Multi-source Benchmark for Drone-based Geo-localization
![image](https://user-images.githubusercontent.com/8390471/192081571-56b84733-238a-45e1-bbf4-988067dbcf51.png)
University-1652 is a multi-view multi-source benchmark for drone-based geo-localization that contains 1652 buildings of 72 universities around the world. We provide images collected from the virtual drone, the satellite and the ground.

[[Paper]](https://arxiv.org/abs/2002.12186)
[[Slide]](http://zdzheng.xyz/ACM-MM-Talk.pdf)
[[Dataset]](https://github.com/layumi/University1652-Baseline)
[[Explore Drone-view Data]](https://github.com/layumi/University1652-Baseline/blob/master/docs/index_files/sample_drone.jpg?raw=true)
[[Explore Satellite-view Data]](https://github.com/layumi/University1652-Baseline/blob/master/docs/index_files/sample_satellite.jpg?raw=true)
[[Explore Street-view Data]](https://github.com/layumi/University1652-Baseline/blob/master/docs/index_files/sample_street.jpg?raw=true)
[[Video Sample]](https://www.youtube.com/embed/dzxXPp8tVn4?vq=hd1080)
[[中文介绍]](https://zhuanlan.zhihu.com/p/110987552)

Task 1: Drone-view target localization. (Drone -> Satellite)} Given one drone-view image or video, the task aims to find the most similar satellite-view image to localize the target building in the satellite view.

Task 2: Drone navigation. (Satellite -> Drone)} Given one satellite-view image, the drone intends to find the most relevant place (drone-view images) that it has passed by. According to its flight history, the drone could be navigated back to the target place.

### 2. DG-Market
<img width="1508" alt="image" src="https://user-images.githubusercontent.com/8390471/192081605-0c8a246f-e54c-41c7-9936-5ba8e22f5192.png">

We provide our generated images and make a large-scale synthetic dataset called DG-Market. This dataset is generated by our DG-Net (https://arxiv.org/abs/1904.07223) and consists of 128,307 images (613MB), about 10 times larger than the training set of original Market-1501 (even much more can be generated with DG-Net). It can be used as a source of unlabeled training dataset for semi-supervised learning. You may download the dataset from [Google Drive](https://drive.google.com/file/d/126Gn90Tzpk3zWp2c7OBYPKc-ZjhptKDo/view?usp=sharing) (or [Baidu Disk](https://pan.baidu.com/s/1n4M6s-qvE08J8SOOWtWfgw) password: qxyh).  

|   |  DG-Market   | Market-1501 (training) |
|---|--------------|-------------|
| #identity| 	-   |  751        |
| #images| 128,307 |  12,936     |

### 3. VSPW: A large-scale dataset for video scene parsing in the wild 
![1001623303486_ pic_hd](https://user-images.githubusercontent.com/12868455/121470723-f6d71180-ca01-11eb-93b5-3db9b6305307.jpg)
[[Project Page]](https://www.vspwdataset.com)

1. Large Scale: 251,632 pixel-level annotated frames from 124 categories, 3,536 videos from 231 scenarios (indoor and  outdoor). 
2. Well-trimmed long-temporal clips: a complete shot lasting 5 seconds on average.
3. Dense annotation: The pixel-level annotations are provided at 15 f/s. 
4. High resolution. Over 96% videos are with high resolutions from 720P to 4K.

### 4. 3D Market-1501
![demo-1](https://user-images.githubusercontent.com/8390471/208151146-b8564829-bd61-484d-850f-61ba75216388.jpg)

You could find the point-cloud format Market-1501 Dataset at https://github.com/layumi/person-reid-3d.


## Awesome Lists

![](https://camo.githubusercontent.com/1131548cf666e1150ebd2a52f44776d539f06324/68747470733a2f2f63646e2e7261776769742e636f6d2f73696e647265736f726875732f617765736f6d652f6d61737465722f6d656469612f6c6f676f2e737667)

- [Awesome Segmentation Domain Adaptation](https://github.com/layumi/Seg-Uncertainty/tree/master/awesome-SegDA)
- [Awesome Vehicle Retrieval](https://github.com/layumi/Vehicle_reID-Collection)
- [Awesome AutoDL](https://github.com/D-X-Y/Awesome-AutoDL)
- [Awesome Fools](https://github.com/layumi/Awesome-Fools)
- [Awesome Geo-localization](https://github.com/layumi/University1652-Baseline/tree/master/State-of-the-art)
